{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from run_experiment import choose_dataset_class,choose_task_classes\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/12543 [00:00<03:58, 52.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [01:48<00:00, 116.01it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2002 [00:00<00:28, 69.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:14<00:00, 137.51it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 20/2077 [00:00<00:10, 198.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:14<00:00, 146.94it/s]\n",
      "[computing labels]:   3%|▎         | 357/12543 [00:00<00:03, 3568.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5397.23it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6960.35it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7508.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# 0. read the embeddings and labesl\n",
    "yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "task = task_class(yaml_args)\n",
    "\n",
    "expt_dataset = dataset_class(yaml_args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25150, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. prepare the label matrix.\n",
    "# train_data = expt_dataset.train_dataset\n",
    "train_data = expt_dataset.dev_dataset\n",
    "\n",
    "labels=[]\n",
    "for obser in train_data.observations:\n",
    "    labels.append(task.labels(obser))\n",
    "\n",
    "all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "all_labels.shape\n",
    "all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "all_labels_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25150, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. prepare the embedding matrix.\n",
    "\n",
    "embeddings=[]\n",
    "for obser in train_data.observations:\n",
    "    embeddings.append(obser.embeddings)\n",
    "\n",
    "all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. compute the corrcoef matrix. \n",
    "# num=204609\n",
    "# corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "corr_mat=np.absolute(corr_mat_whole[0:50,50:])\n",
    "corr_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. output the corr_mat data. dump as a pkl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. output the corr_sum_dim\n",
    "# we compute the corr score of each dim for all the 50 pos labels.\n",
    "dim_sum=[]\n",
    "for i in range(768):\n",
    "    dim_sum.append(np.sum(corr_mat[:,i]))\n",
    "\n",
    "# corr_sum_dim=np.argsort(dim_sum)[::-1]\n",
    "# with open('../SA-dim-files/average_corr_dim.tsv','w') as fout:\n",
    "#     for dim in corr_sum_dim:\n",
    "#         fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. output the weighted_average_corr_dim\n",
    "# we compute the weighted average corr socre of each dim for all the pos labels\n",
    "\n",
    "# 5.1 get the distribution/weights \n",
    "all_labels\n",
    "result = Counter(all_labels)\n",
    "weights = [result[i]/len(all_labels) for i in range(50)]\n",
    "\n",
    "\n",
    "\n",
    "# 5.2 get the weighted score\n",
    "dim_weighted_average=[]\n",
    "for i in range(768):\n",
    "    dim_weighted_average.append(np.average(corr_mat[:,i],weights=weights))\n",
    "    \n",
    "# # 5.3 output the dims\n",
    "# dim_weighted_average_dim=np.argsort(dim_weighted_average)[::-1]\n",
    "# with open('../SA-dim-files/weighted_average_corr_dim.tsv','w') as fout:\n",
    "#     for dim in dim_weighted_average_dim:\n",
    "#         fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 49, 44, 42, 45, 38, 47, 31, 30, 41, 48, 32, 28, 46, 23, 19, 40,\n",
       "       37, 34, 36, 35,  1, 29,  2, 33, 27, 43, 11, 17, 25, 12, 16, 20, 14,\n",
       "       18,  5, 21, 24, 26,  4,  6, 13, 22, 15, 10,  3,  0,  7,  9,  8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13284294234592445, 0.09355864811133201)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[8],weights[9],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. output the max_corr\n",
    "max_val=[]\n",
    "for i in range(768):\n",
    "    max_val.append(np.max(corr_mat[:,i]))\n",
    "\n",
    "dim_max_val=np.argsort(max_val)[::-1]\n",
    "with open('../SA-dim-files/max_corr_dim.tsv','w') as fout:\n",
    "    for dim in dim_weighted_average_dim:\n",
    "        fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.1708928549625375,\n",
       " 0.19993299987706492,\n",
       " 0.4437054850588246,\n",
       " 0.4437054850588246)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dim_sum),np.max(dim_weighted_average),np.max(corr_mat),np.max(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_corr_file(layer=1):\n",
    "\n",
    "    # 0. read the embeddings and labesl\n",
    "    yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "    yaml_args['model']['model_layer']=layer-1\n",
    "\n",
    "    dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "    task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "    task = task_class(yaml_args)\n",
    "\n",
    "    expt_dataset = dataset_class(yaml_args, task)\n",
    "\n",
    "    # 1. prepare the label matrix.\n",
    "    train_data = expt_dataset.train_dataset\n",
    "    labels=[]\n",
    "    for obser in train_data.observations:\n",
    "        labels.append(task.labels(obser))\n",
    "\n",
    "    all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "    all_labels.shape\n",
    "    all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "    all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "    all_labels_mat.shape\n",
    "\n",
    "    # 2. prepare the embedding matrix.\n",
    "\n",
    "    embeddings=[]\n",
    "    for obser in train_data.observations:\n",
    "        embeddings.append(obser.embeddings)\n",
    "\n",
    "    all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "    all_embeddings.shape\n",
    "\n",
    "    # 3. compute the corrcoef matrix. \n",
    "    # num=204609\n",
    "    # corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "    corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "    corr_mat=np.absolute(corr_mat_whole[0:50,50:])\n",
    "    corr_mat.shape\n",
    "\n",
    "    # 4. output the corr_mat data. dump as a pkl.\n",
    "\n",
    "\n",
    "    # 4. output the corr_sum_dim\n",
    "    # we compute the corr score of each dim for all the 50 pos labels.\n",
    "    dim_sum=[]\n",
    "    for i in range(768):\n",
    "        dim_sum.append(np.sum(corr_mat[:,i]))\n",
    "\n",
    "    corr_sum_dim=np.argsort(dim_sum)[::-1]\n",
    "    with open('../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in corr_sum_dim:\n",
    "            fout.write('{} '.format(dim))\n",
    "\n",
    "    # 5. output the weighted_average_corr_dim\n",
    "    # we compute the weighted average corr socre of each dim for all the pos labels\n",
    "\n",
    "    # 5.1 get the distribution/weights \n",
    "    all_labels\n",
    "    result = Counter(all_labels)\n",
    "    weights = [result[i]/len(all_labels) for i in range(50)]\n",
    "\n",
    "\n",
    "\n",
    "    # 5.2 get the weighted score\n",
    "    dim_weighted_average=[]\n",
    "    for i in range(768):\n",
    "        dim_weighted_average.append(np.average(corr_mat[:,i],weights=weights))\n",
    "\n",
    "    # 5.3 output the dims\n",
    "    dim_weighted_average_dim=np.argsort(dim_weighted_average)[::-1]\n",
    "    with open('../SA-dim-files/weighted_average_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in dim_weighted_average_dim:\n",
    "            fout.write('{} '.format(dim))\n",
    "\n",
    "    # 6. output the max_corr\n",
    "    max_val=[]\n",
    "    for i in range(768):\n",
    "        max_val.append(np.max(corr_mat[:,i]))\n",
    "\n",
    "    dim_max_val=np.argsort(max_val)[::-1]\n",
    "    with open('../SA-dim-files/max_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in dim_weighted_average_dim:\n",
    "            fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:36, 338.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 421.56it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 311.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 501.71it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 417.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 519.72it/s]\n",
      "[computing labels]:   3%|▎         | 350/12543 [00:00<00:03, 3494.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5589.88it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 5124.35it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7565.72it/s]\n"
     ]
    }
   ],
   "source": [
    "make_corr_file(layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 30/12543 [00:00<00:42, 292.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 421.01it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 33/2002 [00:00<00:06, 325.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 508.49it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 419.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 510.56it/s]\n",
      "[computing labels]:   3%|▎         | 341/12543 [00:00<00:03, 3407.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5557.51it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6963.88it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7445.42it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:35, 356.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 431.26it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 333.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 507.13it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 423.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 525.57it/s]\n",
      "[computing labels]:   3%|▎         | 384/12543 [00:00<00:03, 3830.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5587.94it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7267.77it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7587.95it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 32/12543 [00:00<00:40, 309.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 429.42it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 335.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 497.44it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 421.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 515.76it/s]\n",
      "[computing labels]:   3%|▎         | 372/12543 [00:00<00:03, 3714.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5328.47it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7151.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7610.98it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 4/12543 [00:00<05:35, 37.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:44<00:00, 55.89it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2002 [00:00<00:28, 71.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:30<00:00, 66.02it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2077 [00:00<00:33, 62.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:31<00:00, 65.72it/s] \n",
      "[computing labels]:   3%|▎         | 336/12543 [00:00<00:03, 3356.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5411.66it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6911.43it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7230.32it/s]\n"
     ]
    }
   ],
   "source": [
    "make_corr_file(layer=3)\n",
    "make_corr_file(layer=6)\n",
    "make_corr_file(layer=9)\n",
    "make_corr_file(layer=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 337.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:30<00:00, 417.41it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 313.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 498.38it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 426.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 513.52it/s]\n",
      "[computing labels]:   3%|▎         | 357/12543 [00:00<00:03, 3566.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5174.07it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6425.51it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7186.58it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 336.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:30<00:00, 415.09it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 312.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 491.10it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 415.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 506.29it/s]\n",
      "[computing labels]:   3%|▎         | 338/12543 [00:00<00:03, 3377.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5471.18it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6947.21it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7497.18it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/12543 [00:00<03:53, 53.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:12<00:00, 65.02it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/2002 [00:00<00:31, 63.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:25<00:00, 79.24it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 14/2077 [00:00<00:20, 102.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:24<00:00, 85.87it/s] \n",
      "[computing labels]:   3%|▎         | 315/12543 [00:00<00:03, 3143.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5261.90it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6791.33it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7254.28it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:30, 46.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [01:56<00:00, 107.76it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 11/2002 [00:00<00:22, 87.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:13<00:00, 144.61it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/2077 [00:00<00:45, 45.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:14<00:00, 144.68it/s]\n",
      "[computing labels]:   3%|▎         | 334/12543 [00:00<00:03, 3339.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5417.84it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6929.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7353.66it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:51<00:00, 54.07it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:28<00:00, 69.40it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:29<00:00, 69.91it/s] \n",
      "[computing labels]:   3%|▎         | 361/12543 [00:00<00:03, 3607.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5441.77it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 4514.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7325.65it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:19, 48.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:35<00:00, 58.24it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/2002 [00:00<00:34, 57.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:27<00:00, 74.02it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2077 [00:00<00:30, 68.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:28<00:00, 71.71it/s] \n",
      "[computing labels]:   6%|▌         | 774/12543 [00:00<00:03, 3809.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5507.04it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7119.98it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7464.66it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 4/12543 [00:00<05:49, 35.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:30<00:00, 59.57it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 3/2002 [00:00<01:10, 28.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:26<00:00, 74.18it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 12/2077 [00:00<00:21, 97.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:27<00:00, 74.21it/s] \n",
      "[computing labels]:   3%|▎         | 334/12543 [00:00<00:03, 3332.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5121.05it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6390.97it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7042.62it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:27, 46.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [02:33<00:00, 81.52it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/2002 [00:00<00:36, 54.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:19<00:00, 102.52it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/2077 [00:00<00:30, 66.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:19<00:00, 108.00it/s]\n",
      "[computing labels]:   3%|▎         | 353/12543 [00:00<00:03, 3529.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5317.11it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 4820.77it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7256.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in [1,2,4,5,7,8,10,11]:\n",
    "    make_corr_file(layer=layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## study the distribution of the embeddings on different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_corr_map(layer=1):\n",
    "\n",
    "    # 0. read the embeddings and labesl\n",
    "    yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "    yaml_args['model']['model_layer']=layer-1\n",
    "\n",
    "    dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "    task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "    task = task_class(yaml_args)\n",
    "\n",
    "    expt_dataset = dataset_class(yaml_args, task)\n",
    "\n",
    "    # 1. prepare the label matrix.\n",
    "    train_data = expt_dataset.train_dataset\n",
    "    labels=[]\n",
    "    for obser in train_data.observations:\n",
    "        labels.append(task.labels(obser))\n",
    "\n",
    "    all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "    all_labels.shape\n",
    "    all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "    all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "    all_labels_mat.shape\n",
    "\n",
    "    # 2. prepare the embedding matrix.\n",
    "\n",
    "    embeddings=[]\n",
    "    for obser in train_data.observations:\n",
    "        embeddings.append(obser.embeddings)\n",
    "\n",
    "    all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "    all_embeddings.shape\n",
    "\n",
    "    # 3. compute the corrcoef matrix. \n",
    "    # num=204609\n",
    "    # corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "    corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "    return corr_mat_whole,all_embeddings,all_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_map_layer_dict=dict.fromkeys([i+1 for i in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 0/12543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 466.10it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 541.20it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2077 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 568.06it/s]\n",
      "[computing labels]:   5%|▌         | 642/12543 [00:00<00:01, 6415.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 8977.60it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11418.57it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11751.56it/s]\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 38/12543 [00:00<00:33, 370.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:27<00:00, 463.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 37/2002 [00:00<00:05, 359.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 569.84it/s]\n",
      "[aligning embeddings]:   2%|▏         | 46/2077 [00:00<00:04, 457.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 2\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 576.12it/s]\n",
      "[computing labels]:   5%|▌         | 674/12543 [00:00<00:01, 6734.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 9047.77it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11405.51it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11776.16it/s]\n",
      "[aligning embeddings]:   0%|          | 41/12543 [00:00<00:30, 403.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 5\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 467.15it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 5\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 545.27it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2077 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 5\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 582.75it/s]\n",
      "[computing labels]:   5%|▌         | 684/12543 [00:00<00:01, 6838.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 9053.32it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11615.61it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11976.94it/s]\n",
      "[aligning embeddings]:   0%|          | 41/12543 [00:00<00:31, 402.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 8\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:27<00:00, 458.82it/s]\n",
      "[aligning embeddings]:   2%|▏         | 37/2002 [00:00<00:05, 360.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 8\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 566.52it/s]\n",
      "[aligning embeddings]:   2%|▏         | 45/2077 [00:00<00:04, 449.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 8\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 574.80it/s]\n",
      "[computing labels]:   5%|▌         | 674/12543 [00:00<00:01, 6730.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 8950.37it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 10829.26it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 10912.31it/s]\n",
      "[aligning embeddings]:   0%|          | 0/12543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 11\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 470.37it/s]\n",
      "[aligning embeddings]:   2%|▏         | 36/2002 [00:00<00:05, 357.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 11\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 560.95it/s]\n",
      "[aligning embeddings]:   2%|▏         | 46/2077 [00:00<00:04, 455.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 11\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 586.35it/s]\n",
      "[computing labels]:   5%|▌         | 650/12543 [00:00<00:01, 6496.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 8378.40it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11778.08it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11809.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in [1,3,6,9,12]:\n",
    "\n",
    "    corr_mat_whole,all_embeddings,all_labels_mat=get_corr_map(layer=layer)\n",
    "    corr_map_layer_dict[layer]={\n",
    "        'corr_mat_whole':corr_mat_whole,\n",
    "        'all_embeddings':all_embeddings,\n",
    "        'all_labels_mat':all_labels_mat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 475.40it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 564.05it/s]\n",
      "[aligning embeddings]:   2%|▏         | 47/2077 [00:00<00:04, 459.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 585.15it/s]\n",
      "[computing labels]:   5%|▌         | 687/12543 [00:00<00:01, 6863.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 9135.94it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11803.32it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7106.95it/s]\n",
      "[aligning embeddings]:   0%|          | 40/12543 [00:00<00:31, 399.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 3\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 470.08it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 3\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 566.20it/s]\n",
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 417.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 3\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 582.08it/s]\n",
      "[computing labels]:   5%|▌         | 687/12543 [00:00<00:01, 6866.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 9165.70it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11547.29it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 12311.73it/s]\n",
      "[aligning embeddings]:   0%|          | 42/12543 [00:00<00:30, 412.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 4\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 474.58it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 4\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 565.60it/s]\n",
      "[aligning embeddings]:   2%|▏         | 47/2077 [00:00<00:04, 459.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 4\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 586.11it/s]\n",
      "[computing labels]:   5%|▌         | 684/12543 [00:00<00:01, 6838.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 8956.25it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11657.67it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11658.16it/s]\n",
      "[aligning embeddings]:   0%|          | 40/12543 [00:00<00:31, 396.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 475.15it/s]\n",
      "[aligning embeddings]:   2%|▏         | 37/2002 [00:00<00:05, 357.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 574.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 47/2077 [00:00<00:04, 460.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 589.76it/s]\n",
      "[computing labels]:   5%|▌         | 683/12543 [00:00<00:01, 6825.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 9162.89it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11581.70it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 12139.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 7\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 470.24it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 7\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 566.43it/s]\n",
      "[aligning embeddings]:   2%|▏         | 47/2077 [00:00<00:04, 458.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 7\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 589.98it/s]\n",
      "[computing labels]:   5%|▌         | 674/12543 [00:00<00:01, 6726.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 8370.43it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11136.98it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11529.46it/s]\n",
      "[aligning embeddings]:   0%|          | 0/12543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 9\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 476.42it/s]\n",
      "[aligning embeddings]:   2%|▏         | 36/2002 [00:00<00:05, 347.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 9\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 575.43it/s]\n",
      "[aligning embeddings]:   2%|▏         | 46/2077 [00:00<00:04, 452.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 9\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 588.79it/s]\n",
      "[computing labels]:   5%|▌         | 657/12543 [00:00<00:01, 6565.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 8729.22it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11170.54it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 11380.83it/s]\n",
      "[aligning embeddings]:   0%|          | 0/12543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 10\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:26<00:00, 478.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 36/2002 [00:00<00:05, 359.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 582.80it/s]\n",
      "[aligning embeddings]:   0%|          | 0/2077 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 10\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 595.93it/s]\n",
      "[computing labels]:   5%|▌         | 679/12543 [00:00<00:01, 6789.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:01<00:00, 9212.56it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 11578.97it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 12175.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in [2,4,5,7,8,10,11]:\n",
    "\n",
    "    corr_mat_whole,all_embeddings,all_labels_mat=get_corr_map(layer=layer)\n",
    "    corr_map_layer_dict[layer]={\n",
    "        'corr_mat_whole':corr_mat_whole,\n",
    "        'all_embeddings':all_embeddings,\n",
    "        'all_labels_mat':all_labels_mat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corr_mat_whole': array([[ 1.00000000e+00, -1.45219888e-02, -1.65926666e-02, ...,\n",
       "          7.42590307e-02, -4.38778696e-02, -7.86450078e-02],\n",
       "        [-1.45219888e-02,  1.00000000e+00, -3.72001856e-03, ...,\n",
       "         -6.48765340e-02,  2.94810034e-02,  1.29795034e-02],\n",
       "        [-1.65926666e-02, -3.72001856e-03,  1.00000000e+00, ...,\n",
       "         -6.04074787e-02, -9.69960175e-03,  3.19866218e-02],\n",
       "        ...,\n",
       "        [ 7.42590307e-02, -6.48765340e-02, -6.04074787e-02, ...,\n",
       "          1.00000000e+00, -7.70285410e-02,  1.03571871e-01],\n",
       "        [-4.38778696e-02,  2.94810034e-02, -9.69960175e-03, ...,\n",
       "         -7.70285410e-02,  1.00000000e+00,  3.38250647e-04],\n",
       "        [-7.86450078e-02,  1.29795034e-02,  3.19866218e-02, ...,\n",
       "          1.03571871e-01,  3.38250647e-04,  1.00000000e+00]]),\n",
       " 'all_embeddings': array([[-0.3103336 , -0.5111257 , -0.6114311 , ...,  0.9943321 ,\n",
       "          0.9872775 ,  0.1236062 ],\n",
       "        [-1.0225046 ,  0.09959992, -0.030162  , ..., -0.9382371 ,\n",
       "          0.5087626 ,  0.09158418],\n",
       "        [-0.32173598, -0.6242337 , -0.4229894 , ...,  0.03532515,\n",
       "         -1.3930013 ,  0.41212052],\n",
       "        ...,\n",
       "        [ 0.69818294, -0.03036093,  0.76832783, ..., -0.61288947,\n",
       "          0.02032898,  0.0433565 ],\n",
       "        [ 0.14268638, -0.27289107, -0.8700371 , ..., -0.39325637,\n",
       "         -1.7417256 ,  1.4515349 ],\n",
       "        [ 0.31693226, -0.30573735,  0.80656874, ..., -0.215245  ,\n",
       "          0.09021679,  0.4818058 ]], dtype=float32),\n",
       " 'all_labels_mat': array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_map_layer_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  1 0.9344386443610329 -0.44777127433163433 0.05445517548586958 0.045042605675376095\n",
      "layer:  3 0.5388708180055392 -0.3891944787099866 0.04346215384211578 0.03648069088313568\n",
      "layer:  6 0.40894972665598045 -0.46745444691679733 0.040155630284109615 0.0335834003641113\n",
      "layer:  9 0.29406288335072756 -0.5437658693230691 0.037730351200416595 0.03145815035875132\n",
      "layer:  12 0.8915102125630212 -0.9125223359281933 0.062358668342203924 0.050669063341169114\n"
     ]
    }
   ],
   "source": [
    "for layer in [1,3,6,9,12]:\n",
    "#     emb_mat = np.absolute(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    emb_mat = corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int)\n",
    "\n",
    "    print('layer: ',layer,np.max(emb_mat),np.min(emb_mat),np.mean(np.absolute(emb_mat)),np.median(np.absolute(emb_mat)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dim_from_file(file):\n",
    "    with open(file,'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "        line=lines[0].strip().split()\n",
    "        dims=[int(line[i]) for i in range(len(line))]\n",
    "    return dims\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer\t1\tmax-corr\t0.9344386443610329\tmean-corr\t0.05445517548586958 \n",
      "layer\t2\tmax-corr\t0.6790318898745872\tmean-corr\t0.045984754656778135 \n",
      "layer\t3\tmax-corr\t0.5388708180055392\tmean-corr\t0.04346215384211578 \n",
      "layer\t4\tmax-corr\t0.45349813123061394\tmean-corr\t0.04306083598380115 \n",
      "layer\t5\tmax-corr\t0.4081479121011531\tmean-corr\t0.0412659191877765 \n",
      "layer\t6\tmax-corr\t0.46745444691679733\tmean-corr\t0.040155630284109615 \n",
      "layer\t7\tmax-corr\t0.5212013234748754\tmean-corr\t0.03904785366661185 \n",
      "layer\t8\tmax-corr\t0.5658149898225272\tmean-corr\t0.03822041388538651 \n",
      "layer\t9\tmax-corr\t0.5437658693230691\tmean-corr\t0.037730351200416595 \n",
      "layer\t10\tmax-corr\t0.7716978112891458\tmean-corr\t0.03926388344173654 \n",
      "layer\t11\tmax-corr\t0.7371881250346611\tmean-corr\t0.041128799093268414 \n",
      "layer\t12\tmax-corr\t0.9125223359281933\tmean-corr\t0.062358668342203924 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for layer in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "    k=4\n",
    "#     emb_mat = np.absolute(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    emb_mat = np.array(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    dim_file = '../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer)\n",
    "    print('layer\\t{}\\tmax-corr\\t{}\\tmean-corr\\t{} '.format(layer,np.max(np.absolute(emb_mat)),np.mean(np.absolute(emb_mat)),))\n",
    "#     print('layer: ',layer,np.max(emb_mat),np.min(emb_mat),np.max(np.absolute(emb_mat)),np.mean(np.absolute(emb_mat)),np.median(np.absolute(emb_mat)))\n",
    "    dims=get_top_dim_from_file(file=dim_file)\n",
    "    top_emb_mat = emb_mat[dims[0:k]]\n",
    "    top_emb_mat = top_emb_mat[:,dims[0:k]]\n",
    "#     print('layer: ',layer,np.max(top_emb_mat),np.min(top_emb_mat),np.max(np.absolute(top_emb_mat)),np.mean(np.absolute(top_emb_mat)),np.median(np.absolute(top_emb_mat)))\n",
    "#     print(top_emb_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer\t1\tmax-corr\t0.9344386443610329\tmean-corr\t0.33377593890588725 \n",
      "layer\t2\tmax-corr\t0.6790318898745872\tmean-corr\t0.2774187195087825 \n",
      "layer\t3\tmax-corr\t0.5388708180055392\tmean-corr\t0.13675694176581563 \n",
      "layer\t4\tmax-corr\t0.45349813123061394\tmean-corr\t0.11756843120368758 \n",
      "layer\t5\tmax-corr\t0.3974267527739602\tmean-corr\t0.14666693370442338 \n",
      "layer\t6\tmax-corr\t0.40894972665598045\tmean-corr\t0.1272581291682241 \n",
      "layer\t7\tmax-corr\t0.3823351255817417\tmean-corr\t0.12069361046873045 \n",
      "layer\t8\tmax-corr\t0.31555230120604344\tmean-corr\t0.11244593569842101 \n",
      "layer\t9\tmax-corr\t0.2464554089840436\tmean-corr\t0.1098964963130136 \n",
      "layer\t10\tmax-corr\t0.2551217223482187\tmean-corr\t0.12510328069650808 \n",
      "layer\t11\tmax-corr\t0.21970662681363654\tmean-corr\t0.08209500617508024 \n",
      "layer\t12\tmax-corr\t0.18384732208722365\tmean-corr\t0.06815413873166584 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for layer in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "    k=5\n",
    "#     emb_mat = np.absolute(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    emb_mat = np.array(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    dim_file = '../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer)\n",
    "#     print('layer: ',layer,np.max(emb_mat),np.min(emb_mat),np.max(np.absolute(emb_mat)),np.mean(np.absolute(emb_mat)),np.median(np.absolute(emb_mat)))\n",
    "    dims=get_top_dim_from_file(file=dim_file)\n",
    "    top_emb_mat = emb_mat[dims[0:k]]\n",
    "    top_emb_mat = top_emb_mat[:,dims[0:k]]\n",
    "    print('layer\\t{}\\tmax-corr\\t{}\\tmean-corr\\t{} '.format(layer,np.max(np.absolute(top_emb_mat)),np.mean(np.absolute(top_emb_mat)),))\n",
    "#     print('layer: ',layer,np.max(top_emb_mat),np.min(top_emb_mat),np.max(np.absolute(top_emb_mat)),np.mean(np.absolute(top_emb_mat)),np.median(np.absolute(top_emb_mat)))\n",
    "#     print(top_emb_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0406\n",
    "\n",
    "we want to follow 'Identifying and controlling important neurons in neural machine translation'.  find the relationship of different dim from layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204609, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_map_layer_dict[1]['all_embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat_layer = np.corrcoef(corr_map_layer_dict[1]['all_embeddings'].T,corr_map_layer_dict[2]['all_embeddings'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat_layer=corr_mat_layer[0:768,768:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90742854, -0.06963579,  0.05410695, ...,  0.04343287,\n",
       "         0.05730724, -0.08297921],\n",
       "       [-0.1048294 ,  0.88629352, -0.01495206, ..., -0.04304126,\n",
       "         0.03685551, -0.03654008],\n",
       "       [ 0.08426971, -0.04959946,  0.90144014, ...,  0.0309325 ,\n",
       "        -0.08627646,  0.02896551],\n",
       "       ...,\n",
       "       [ 0.05949167, -0.00652122,  0.04063978, ...,  0.91046108,\n",
       "        -0.09323254,  0.12233507],\n",
       "       [ 0.01851293,  0.03472938, -0.04887031, ..., -0.06296976,\n",
       "         0.93806145, -0.01613298],\n",
       "       [-0.03688275, -0.05995711,  0.00956521, ...,  0.08089813,\n",
       "        -0.00120516,  0.90118942]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat_layer=np.absolute(corr_mat_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1785848206178393e-07, 0.983326742932099)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(corr_mat_layer,axis=1)\n",
    "np.min(corr_mat_layer),np.max(corr_mat_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build a MaxCorr for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MaxCorr_score_for_one_layer(corr_map_layer_dict,cur_layer):\n",
    "    # 1. get the corr_mat for cur_layer and 11 other layers\n",
    "    layers = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    layers.remove(cur_layer)\n",
    "    corr_mat_layers={}\n",
    "    for layer in layers:\n",
    "        corr_mat_layer = np.corrcoef(corr_map_layer_dict[cur_layer]['all_embeddings'].T,corr_map_layer_dict[layer]['all_embeddings'].T)\n",
    "        corr_mat_layer=np.absolute(corr_mat_layer[0:768,768:])\n",
    "        corr_mat_layers[layer] = corr_mat_layer\n",
    "        \n",
    "    # 2. get the MaxCorr score\n",
    "    all_corr_mat=[np.max(corr_mat_layers[layer],axis=1) for layer in layers]    \n",
    "    all_corr_mat = np.array(all_corr_mat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # we first get the max for one layer, and find the max over layers\n",
    "    MaxCorr_sort=np.argsort(np.max(all_corr_mat,axis=0))[::-1]\n",
    "        # we first get the max for one layer, and find the min over layers\n",
    "    MinCorr_sort=np.argsort(np.min(all_corr_mat,axis=0))[::-1]\n",
    "    \n",
    "    mean_corr_mat=corr_mat_layers[layers[0]]\n",
    "    for layer in layers[1:]:\n",
    "        mean_corr_mat+=corr_mat_layers[layer]\n",
    "    # we first get the mean all layer, and find the max over layers\n",
    "    MeanCorr_sort=np.argsort(np.max(mean_corr_mat,axis=1))[::-1]\n",
    "\n",
    "    \n",
    "    return corr_mat_layers,MaxCorr_sort,MinCorr_sort,MeanCorr_sort\n",
    "        \n",
    "def write_file(cur_layer,MaxCorr_sort,MinCorr_sort,MeanCorr_sort):\n",
    "    with open('../SA-dim-files/Bau_MaxCorr_dim_layer_{}.tsv'.format(cur_layer),'w') as fout:\n",
    "        for dim in MaxCorr_sort:\n",
    "            fout.write('{} '.format(dim))\n",
    "    with open('../SA-dim-files/Bau_MinCorr_dim_layer_{}.tsv'.format(cur_layer),'w') as fout:\n",
    "        for dim in MinCorr_sort:\n",
    "            fout.write('{} '.format(dim))\n",
    "    with open('../SA-dim-files/Bau_MeanCorr_dim_layer_{}.tsv'.format(cur_layer),'w') as fout:\n",
    "        for dim in MeanCorr_sort:\n",
    "            fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 1\n",
      "finish 2\n",
      "finish 3\n",
      "finish 4\n",
      "finish 5\n",
      "finish 6\n",
      "finish 7\n",
      "finish 8\n",
      "finish 9\n",
      "finish 10\n",
      "finish 11\n",
      "finish 12\n"
     ]
    }
   ],
   "source": [
    "for cur_layer in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "    corr_mat_layers,MaxCorr_sort,MinCorr_sort,MeanCorr_sort = get_MaxCorr_score_for_one_layer(corr_map_layer_dict,cur_layer)\n",
    "    write_file(cur_layer,MaxCorr_sort,MinCorr_sort,MeanCorr_sort)\n",
    "    print('finish',cur_layer)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
