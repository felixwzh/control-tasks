{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from run_experiment import choose_dataset_class,choose_task_classes\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/12543 [00:00<03:45, 55.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [02:54<00:00, 71.90it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/2002 [00:00<00:29, 66.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:21<00:00, 92.44it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 11/2077 [00:00<00:18, 109.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:22<00:00, 93.59it/s] \n",
      "[computing labels]:   3%|▎         | 351/12543 [00:00<00:03, 3502.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5237.47it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7055.62it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7065.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# 0. read the embeddings and labesl\n",
    "yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "task = task_class(yaml_args)\n",
    "\n",
    "expt_dataset = dataset_class(yaml_args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204609, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. prepare the label matrix.\n",
    "train_data = expt_dataset.train_dataset\n",
    "labels=[]\n",
    "for obser in train_data.observations:\n",
    "    labels.append(task.labels(obser))\n",
    "\n",
    "all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "all_labels.shape\n",
    "all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "all_labels_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204609, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. prepare the embedding matrix.\n",
    "\n",
    "embeddings=[]\n",
    "for obser in train_data.observations:\n",
    "    embeddings.append(obser.embeddings)\n",
    "\n",
    "all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. compute the corrcoef matrix. \n",
    "# num=204609\n",
    "# corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "corr_mat=np.absolute(corr_mat_whole[0:50,50:])\n",
    "corr_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. output the corr_mat data. dump as a pkl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. output the corr_sum_dim\n",
    "# we compute the corr score of each dim for all the 50 pos labels.\n",
    "dim_sum=[]\n",
    "for i in range(768):\n",
    "    dim_sum.append(np.sum(corr_mat[:,i]))\n",
    "\n",
    "corr_sum_dim=np.argsort(dim_sum)[::-1]\n",
    "with open('../SA-dim-files/average_corr_dim.tsv','w') as fout:\n",
    "    for dim in corr_sum_dim:\n",
    "        fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. output the weighted_average_corr_dim\n",
    "# we compute the weighted average corr socre of each dim for all the pos labels\n",
    "\n",
    "# 5.1 get the distribution/weights \n",
    "all_labels\n",
    "result = Counter(all_labels)\n",
    "weights = [result[i]/len(all_labels) for i in range(50)]\n",
    "\n",
    "\n",
    "\n",
    "# 5.2 get the weighted score\n",
    "dim_weighted_average=[]\n",
    "for i in range(768):\n",
    "    dim_weighted_average.append(np.average(corr_mat[:,i],weights=weights))\n",
    "    \n",
    "# 5.3 output the dims\n",
    "dim_weighted_average_dim=np.argsort(dim_weighted_average)[::-1]\n",
    "with open('../SA-dim-files/weighted_average_corr_dim.tsv','w') as fout:\n",
    "    for dim in dim_weighted_average_dim:\n",
    "        fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. output the max_corr\n",
    "max_val=[]\n",
    "for i in range(768):\n",
    "    max_val.append(np.max(corr_mat[:,i]))\n",
    "\n",
    "dim_max_val=np.argsort(max_val)[::-1]\n",
    "with open('../SA-dim-files/max_corr_dim.tsv','w') as fout:\n",
    "    for dim in dim_weighted_average_dim:\n",
    "        fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.1708928549625375,\n",
       " 0.19993299987706492,\n",
       " 0.4437054850588246,\n",
       " 0.4437054850588246)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dim_sum),np.max(dim_weighted_average),np.max(corr_mat),np.max(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_corr_file(layer=1):\n",
    "\n",
    "    # 0. read the embeddings and labesl\n",
    "    yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "    yaml_args['model']['model_layer']=layer-1\n",
    "\n",
    "    dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "    task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "    task = task_class(yaml_args)\n",
    "\n",
    "    expt_dataset = dataset_class(yaml_args, task)\n",
    "\n",
    "    # 1. prepare the label matrix.\n",
    "    train_data = expt_dataset.train_dataset\n",
    "    labels=[]\n",
    "    for obser in train_data.observations:\n",
    "        labels.append(task.labels(obser))\n",
    "\n",
    "    all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "    all_labels.shape\n",
    "    all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "    all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "    all_labels_mat.shape\n",
    "\n",
    "    # 2. prepare the embedding matrix.\n",
    "\n",
    "    embeddings=[]\n",
    "    for obser in train_data.observations:\n",
    "        embeddings.append(obser.embeddings)\n",
    "\n",
    "    all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "    all_embeddings.shape\n",
    "\n",
    "    # 3. compute the corrcoef matrix. \n",
    "    # num=204609\n",
    "    # corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "    corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "    corr_mat=np.absolute(corr_mat_whole[0:50,50:])\n",
    "    corr_mat.shape\n",
    "\n",
    "    # 4. output the corr_mat data. dump as a pkl.\n",
    "\n",
    "\n",
    "    # 4. output the corr_sum_dim\n",
    "    # we compute the corr score of each dim for all the 50 pos labels.\n",
    "    dim_sum=[]\n",
    "    for i in range(768):\n",
    "        dim_sum.append(np.sum(corr_mat[:,i]))\n",
    "\n",
    "    corr_sum_dim=np.argsort(dim_sum)[::-1]\n",
    "    with open('../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in corr_sum_dim:\n",
    "            fout.write('{} '.format(dim))\n",
    "\n",
    "    # 5. output the weighted_average_corr_dim\n",
    "    # we compute the weighted average corr socre of each dim for all the pos labels\n",
    "\n",
    "    # 5.1 get the distribution/weights \n",
    "    all_labels\n",
    "    result = Counter(all_labels)\n",
    "    weights = [result[i]/len(all_labels) for i in range(50)]\n",
    "\n",
    "\n",
    "\n",
    "    # 5.2 get the weighted score\n",
    "    dim_weighted_average=[]\n",
    "    for i in range(768):\n",
    "        dim_weighted_average.append(np.average(corr_mat[:,i],weights=weights))\n",
    "\n",
    "    # 5.3 output the dims\n",
    "    dim_weighted_average_dim=np.argsort(dim_weighted_average)[::-1]\n",
    "    with open('../SA-dim-files/weighted_average_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in dim_weighted_average_dim:\n",
    "            fout.write('{} '.format(dim))\n",
    "\n",
    "    # 6. output the max_corr\n",
    "    max_val=[]\n",
    "    for i in range(768):\n",
    "        max_val.append(np.max(corr_mat[:,i]))\n",
    "\n",
    "    dim_max_val=np.argsort(max_val)[::-1]\n",
    "    with open('../SA-dim-files/max_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in dim_weighted_average_dim:\n",
    "            fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:36, 338.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 421.56it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 311.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 501.71it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 417.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 519.72it/s]\n",
      "[computing labels]:   3%|▎         | 350/12543 [00:00<00:03, 3494.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5589.88it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 5124.35it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7565.72it/s]\n"
     ]
    }
   ],
   "source": [
    "make_corr_file(layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 30/12543 [00:00<00:42, 292.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 421.01it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 33/2002 [00:00<00:06, 325.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 508.49it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 419.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 510.56it/s]\n",
      "[computing labels]:   3%|▎         | 341/12543 [00:00<00:03, 3407.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5557.51it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6963.88it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7445.42it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:35, 356.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 431.26it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 333.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 507.13it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 423.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 525.57it/s]\n",
      "[computing labels]:   3%|▎         | 384/12543 [00:00<00:03, 3830.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5587.94it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7267.77it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7587.95it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 32/12543 [00:00<00:40, 309.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 429.42it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 335.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 497.44it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 421.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 515.76it/s]\n",
      "[computing labels]:   3%|▎         | 372/12543 [00:00<00:03, 3714.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5328.47it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7151.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7610.98it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 4/12543 [00:00<05:35, 37.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:44<00:00, 55.89it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2002 [00:00<00:28, 71.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:30<00:00, 66.02it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2077 [00:00<00:33, 62.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:31<00:00, 65.72it/s] \n",
      "[computing labels]:   3%|▎         | 336/12543 [00:00<00:03, 3356.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5411.66it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6911.43it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7230.32it/s]\n"
     ]
    }
   ],
   "source": [
    "make_corr_file(layer=3)\n",
    "make_corr_file(layer=6)\n",
    "make_corr_file(layer=9)\n",
    "make_corr_file(layer=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 337.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:30<00:00, 417.41it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 313.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 498.38it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 426.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 513.52it/s]\n",
      "[computing labels]:   3%|▎         | 357/12543 [00:00<00:03, 3566.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5174.07it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6425.51it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7186.58it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 336.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:30<00:00, 415.09it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 312.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 491.10it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 415.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 506.29it/s]\n",
      "[computing labels]:   3%|▎         | 338/12543 [00:00<00:03, 3377.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5471.18it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6947.21it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7497.18it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/12543 [00:00<03:53, 53.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:12<00:00, 65.02it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/2002 [00:00<00:31, 63.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:25<00:00, 79.24it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 14/2077 [00:00<00:20, 102.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:24<00:00, 85.87it/s] \n",
      "[computing labels]:   3%|▎         | 315/12543 [00:00<00:03, 3143.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5261.90it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6791.33it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7254.28it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:30, 46.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [01:56<00:00, 107.76it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 11/2002 [00:00<00:22, 87.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:13<00:00, 144.61it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/2077 [00:00<00:45, 45.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:14<00:00, 144.68it/s]\n",
      "[computing labels]:   3%|▎         | 334/12543 [00:00<00:03, 3339.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5417.84it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6929.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7353.66it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:51<00:00, 54.07it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:28<00:00, 69.40it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:29<00:00, 69.91it/s] \n",
      "[computing labels]:   3%|▎         | 361/12543 [00:00<00:03, 3607.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5441.77it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 4514.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7325.65it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:19, 48.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:35<00:00, 58.24it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/2002 [00:00<00:34, 57.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:27<00:00, 74.02it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2077 [00:00<00:30, 68.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:28<00:00, 71.71it/s] \n",
      "[computing labels]:   6%|▌         | 774/12543 [00:00<00:03, 3809.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5507.04it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7119.98it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7464.66it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 4/12543 [00:00<05:49, 35.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:  49%|████▉     | 6151/12543 [01:39<02:39, 40.19it/s] "
     ]
    }
   ],
   "source": [
    "for layer in [1,2,4,5,7,8,10,11]:\n",
    "    make_corr_file(layer=layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
