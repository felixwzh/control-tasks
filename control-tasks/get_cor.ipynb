{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from run_experiment import choose_dataset_class,choose_task_classes\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/12543 [00:00<03:58, 52.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [01:48<00:00, 116.01it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2002 [00:00<00:28, 69.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:14<00:00, 137.51it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 20/2077 [00:00<00:10, 198.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:14<00:00, 146.94it/s]\n",
      "[computing labels]:   3%|▎         | 357/12543 [00:00<00:03, 3568.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5397.23it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6960.35it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7508.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# 0. read the embeddings and labesl\n",
    "yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "task = task_class(yaml_args)\n",
    "\n",
    "expt_dataset = dataset_class(yaml_args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25150, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. prepare the label matrix.\n",
    "# train_data = expt_dataset.train_dataset\n",
    "train_data = expt_dataset.dev_dataset\n",
    "\n",
    "labels=[]\n",
    "for obser in train_data.observations:\n",
    "    labels.append(task.labels(obser))\n",
    "\n",
    "all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "all_labels.shape\n",
    "all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "all_labels_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25150, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. prepare the embedding matrix.\n",
    "\n",
    "embeddings=[]\n",
    "for obser in train_data.observations:\n",
    "    embeddings.append(obser.embeddings)\n",
    "\n",
    "all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. compute the corrcoef matrix. \n",
    "# num=204609\n",
    "# corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "corr_mat=np.absolute(corr_mat_whole[0:50,50:])\n",
    "corr_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. output the corr_mat data. dump as a pkl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. output the corr_sum_dim\n",
    "# we compute the corr score of each dim for all the 50 pos labels.\n",
    "dim_sum=[]\n",
    "for i in range(768):\n",
    "    dim_sum.append(np.sum(corr_mat[:,i]))\n",
    "\n",
    "# corr_sum_dim=np.argsort(dim_sum)[::-1]\n",
    "# with open('../SA-dim-files/average_corr_dim.tsv','w') as fout:\n",
    "#     for dim in corr_sum_dim:\n",
    "#         fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. output the weighted_average_corr_dim\n",
    "# we compute the weighted average corr socre of each dim for all the pos labels\n",
    "\n",
    "# 5.1 get the distribution/weights \n",
    "all_labels\n",
    "result = Counter(all_labels)\n",
    "weights = [result[i]/len(all_labels) for i in range(50)]\n",
    "\n",
    "\n",
    "\n",
    "# 5.2 get the weighted score\n",
    "dim_weighted_average=[]\n",
    "for i in range(768):\n",
    "    dim_weighted_average.append(np.average(corr_mat[:,i],weights=weights))\n",
    "    \n",
    "# # 5.3 output the dims\n",
    "# dim_weighted_average_dim=np.argsort(dim_weighted_average)[::-1]\n",
    "# with open('../SA-dim-files/weighted_average_corr_dim.tsv','w') as fout:\n",
    "#     for dim in dim_weighted_average_dim:\n",
    "#         fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 49, 44, 42, 45, 38, 47, 31, 30, 41, 48, 32, 28, 46, 23, 19, 40,\n",
       "       37, 34, 36, 35,  1, 29,  2, 33, 27, 43, 11, 17, 25, 12, 16, 20, 14,\n",
       "       18,  5, 21, 24, 26,  4,  6, 13, 22, 15, 10,  3,  0,  7,  9,  8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13284294234592445, 0.09355864811133201)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[8],weights[9],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. output the max_corr\n",
    "max_val=[]\n",
    "for i in range(768):\n",
    "    max_val.append(np.max(corr_mat[:,i]))\n",
    "\n",
    "dim_max_val=np.argsort(max_val)[::-1]\n",
    "with open('../SA-dim-files/max_corr_dim.tsv','w') as fout:\n",
    "    for dim in dim_weighted_average_dim:\n",
    "        fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.1708928549625375,\n",
       " 0.19993299987706492,\n",
       " 0.4437054850588246,\n",
       " 0.4437054850588246)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dim_sum),np.max(dim_weighted_average),np.max(corr_mat),np.max(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_corr_file(layer=1):\n",
    "\n",
    "    # 0. read the embeddings and labesl\n",
    "    yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "    yaml_args['model']['model_layer']=layer-1\n",
    "\n",
    "    dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "    task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "    task = task_class(yaml_args)\n",
    "\n",
    "    expt_dataset = dataset_class(yaml_args, task)\n",
    "\n",
    "    # 1. prepare the label matrix.\n",
    "    train_data = expt_dataset.train_dataset\n",
    "    labels=[]\n",
    "    for obser in train_data.observations:\n",
    "        labels.append(task.labels(obser))\n",
    "\n",
    "    all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "    all_labels.shape\n",
    "    all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "    all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "    all_labels_mat.shape\n",
    "\n",
    "    # 2. prepare the embedding matrix.\n",
    "\n",
    "    embeddings=[]\n",
    "    for obser in train_data.observations:\n",
    "        embeddings.append(obser.embeddings)\n",
    "\n",
    "    all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "    all_embeddings.shape\n",
    "\n",
    "    # 3. compute the corrcoef matrix. \n",
    "    # num=204609\n",
    "    # corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "    corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "    corr_mat=np.absolute(corr_mat_whole[0:50,50:])\n",
    "    corr_mat.shape\n",
    "\n",
    "    # 4. output the corr_mat data. dump as a pkl.\n",
    "\n",
    "\n",
    "    # 4. output the corr_sum_dim\n",
    "    # we compute the corr score of each dim for all the 50 pos labels.\n",
    "    dim_sum=[]\n",
    "    for i in range(768):\n",
    "        dim_sum.append(np.sum(corr_mat[:,i]))\n",
    "\n",
    "    corr_sum_dim=np.argsort(dim_sum)[::-1]\n",
    "    with open('../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in corr_sum_dim:\n",
    "            fout.write('{} '.format(dim))\n",
    "\n",
    "    # 5. output the weighted_average_corr_dim\n",
    "    # we compute the weighted average corr socre of each dim for all the pos labels\n",
    "\n",
    "    # 5.1 get the distribution/weights \n",
    "    all_labels\n",
    "    result = Counter(all_labels)\n",
    "    weights = [result[i]/len(all_labels) for i in range(50)]\n",
    "\n",
    "\n",
    "\n",
    "    # 5.2 get the weighted score\n",
    "    dim_weighted_average=[]\n",
    "    for i in range(768):\n",
    "        dim_weighted_average.append(np.average(corr_mat[:,i],weights=weights))\n",
    "\n",
    "    # 5.3 output the dims\n",
    "    dim_weighted_average_dim=np.argsort(dim_weighted_average)[::-1]\n",
    "    with open('../SA-dim-files/weighted_average_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in dim_weighted_average_dim:\n",
    "            fout.write('{} '.format(dim))\n",
    "\n",
    "    # 6. output the max_corr\n",
    "    max_val=[]\n",
    "    for i in range(768):\n",
    "        max_val.append(np.max(corr_mat[:,i]))\n",
    "\n",
    "    dim_max_val=np.argsort(max_val)[::-1]\n",
    "    with open('../SA-dim-files/max_corr_dim_layer_{}.tsv'.format(layer),'w') as fout:\n",
    "        for dim in dim_weighted_average_dim:\n",
    "            fout.write('{} '.format(dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:36, 338.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 421.56it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 311.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 501.71it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 417.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 519.72it/s]\n",
      "[computing labels]:   3%|▎         | 350/12543 [00:00<00:03, 3494.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5589.88it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 5124.35it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7565.72it/s]\n"
     ]
    }
   ],
   "source": [
    "make_corr_file(layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 30/12543 [00:00<00:42, 292.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 421.01it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 33/2002 [00:00<00:06, 325.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 508.49it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 419.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 510.56it/s]\n",
      "[computing labels]:   3%|▎         | 341/12543 [00:00<00:03, 3407.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5557.51it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6963.88it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7445.42it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:35, 356.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 431.26it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 333.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 507.13it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 423.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 525.57it/s]\n",
      "[computing labels]:   3%|▎         | 384/12543 [00:00<00:03, 3830.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5587.94it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7267.77it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7587.95it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 32/12543 [00:00<00:40, 309.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 429.42it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 335.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 497.44it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 421.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 515.76it/s]\n",
      "[computing labels]:   3%|▎         | 372/12543 [00:00<00:03, 3714.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5328.47it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7151.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7610.98it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 4/12543 [00:00<05:35, 37.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:44<00:00, 55.89it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2002 [00:00<00:28, 71.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:30<00:00, 66.02it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2077 [00:00<00:33, 62.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:31<00:00, 65.72it/s] \n",
      "[computing labels]:   3%|▎         | 336/12543 [00:00<00:03, 3356.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5411.66it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6911.43it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7230.32it/s]\n"
     ]
    }
   ],
   "source": [
    "make_corr_file(layer=3)\n",
    "make_corr_file(layer=6)\n",
    "make_corr_file(layer=9)\n",
    "make_corr_file(layer=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 337.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:30<00:00, 417.41it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 313.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 498.38it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 426.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 513.52it/s]\n",
      "[computing labels]:   3%|▎         | 357/12543 [00:00<00:03, 3566.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5174.07it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6425.51it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7186.58it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 336.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:30<00:00, 415.09it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 312.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 491.10it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 415.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 506.29it/s]\n",
      "[computing labels]:   3%|▎         | 338/12543 [00:00<00:03, 3377.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5471.18it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6947.21it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7497.18it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/12543 [00:00<03:53, 53.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:12<00:00, 65.02it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/2002 [00:00<00:31, 63.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:25<00:00, 79.24it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 14/2077 [00:00<00:20, 102.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:24<00:00, 85.87it/s] \n",
      "[computing labels]:   3%|▎         | 315/12543 [00:00<00:03, 3143.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5261.90it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6791.33it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7254.28it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:30, 46.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [01:56<00:00, 107.76it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 11/2002 [00:00<00:22, 87.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:13<00:00, 144.61it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/2077 [00:00<00:45, 45.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:14<00:00, 144.68it/s]\n",
      "[computing labels]:   3%|▎         | 334/12543 [00:00<00:03, 3339.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5417.84it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6929.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7353.66it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:51<00:00, 54.07it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:28<00:00, 69.40it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 6\n",
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:29<00:00, 69.91it/s] \n",
      "[computing labels]:   3%|▎         | 361/12543 [00:00<00:03, 3607.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5441.77it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 4514.16it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7325.65it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:19, 48.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:35<00:00, 58.24it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/2002 [00:00<00:34, 57.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:27<00:00, 74.02it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 8/2077 [00:00<00:30, 68.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:28<00:00, 71.71it/s] \n",
      "[computing labels]:   6%|▌         | 774/12543 [00:00<00:03, 3809.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5507.04it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7119.98it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7464.66it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 4/12543 [00:00<05:49, 35.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [03:30<00:00, 59.57it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 3/2002 [00:00<01:10, 28.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:26<00:00, 74.18it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   1%|          | 12/2077 [00:00<00:21, 97.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:27<00:00, 74.21it/s] \n",
      "[computing labels]:   3%|▎         | 334/12543 [00:00<00:03, 3332.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5121.05it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6390.97it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7042.62it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 5/12543 [00:00<04:27, 46.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [02:33<00:00, 81.52it/s] \n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 6/2002 [00:00<00:36, 54.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:19<00:00, 102.52it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 7/2077 [00:00<00:30, 66.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:19<00:00, 108.00it/s]\n",
      "[computing labels]:   3%|▎         | 353/12543 [00:00<00:03, 3529.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5317.11it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 4820.77it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7256.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in [1,2,4,5,7,8,10,11]:\n",
    "    make_corr_file(layer=layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## study the distribution of the embeddings on different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_corr_map(layer=1):\n",
    "\n",
    "    # 0. read the embeddings and labesl\n",
    "    yaml_args= yaml.load(open('../SA-config/a-sample-pos-get_cor.yaml'))\n",
    "\n",
    "    yaml_args['model']['model_layer']=layer-1\n",
    "\n",
    "    dataset_class = choose_dataset_class(yaml_args)\n",
    "\n",
    "    task_class, reporter_class, loss_class = choose_task_classes(yaml_args)\n",
    "\n",
    "    task = task_class(yaml_args)\n",
    "\n",
    "    expt_dataset = dataset_class(yaml_args, task)\n",
    "\n",
    "    # 1. prepare the label matrix.\n",
    "    train_data = expt_dataset.train_dataset\n",
    "    labels=[]\n",
    "    for obser in train_data.observations:\n",
    "        labels.append(task.labels(obser))\n",
    "\n",
    "    all_labels = torch.cat(labels, 0).numpy().astype(int)\n",
    "\n",
    "    all_labels.shape\n",
    "    all_labels_mat= np.zeros((all_labels.size, all_labels.max()+1))\n",
    "    all_labels_mat[np.arange(all_labels.size),all_labels] = 1\n",
    "    all_labels_mat.shape\n",
    "\n",
    "    # 2. prepare the embedding matrix.\n",
    "\n",
    "    embeddings=[]\n",
    "    for obser in train_data.observations:\n",
    "        embeddings.append(obser.embeddings)\n",
    "\n",
    "    all_embeddings = torch.cat(embeddings, 0).numpy()\n",
    "\n",
    "    all_embeddings.shape\n",
    "\n",
    "    # 3. compute the corrcoef matrix. \n",
    "    # num=204609\n",
    "    # corr_mat_whole = np.corrcoef(all_labels_mat[0:num].T, all_embeddings[0:num].T)\n",
    "    corr_mat_whole = np.corrcoef(all_labels_mat.T, all_embeddings.T)\n",
    "    return corr_mat_whole,all_embeddings,all_labels_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_map_layer_dict=dict.fromkeys([i+1 for i in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "[aligning embeddings]:   0%|          | 35/12543 [00:00<00:36, 346.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 423.09it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 313.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:04<00:00, 491.20it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:05, 404.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 512.57it/s]\n",
      "[computing labels]:   3%|▎         | 332/12543 [00:00<00:03, 3319.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5096.50it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6825.21it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7072.89it/s]\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:36, 339.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 422.88it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 314.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 501.26it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 37/2077 [00:00<00:05, 368.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 512.21it/s]\n",
      "[computing labels]:   3%|▎         | 342/12543 [00:00<00:03, 3417.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5286.39it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6780.95it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7154.05it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 35/12543 [00:00<00:36, 344.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 427.39it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 33/2002 [00:00<00:06, 323.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 501.36it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 417.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 510.67it/s]\n",
      "[computing labels]:   3%|▎         | 333/12543 [00:00<00:03, 3323.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5250.12it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6795.95it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 6977.15it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:35, 355.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 425.71it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 316.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 502.29it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 417.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:04<00:00, 516.50it/s]\n",
      "[computing labels]:   3%|▎         | 332/12543 [00:00<00:03, 3315.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5285.85it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6700.11it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7241.54it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:35, 356.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 427.83it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 31/2002 [00:00<00:06, 308.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 500.65it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 415.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 520.62it/s]\n",
      "[computing labels]:   3%|▎         | 332/12543 [00:00<00:03, 3316.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5125.60it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6625.20it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7096.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in [1,3,6,9,12]:\n",
    "\n",
    "    corr_mat_whole,all_embeddings,all_labels_mat=get_corr_map(layer=layer)\n",
    "    corr_map_layer_dict[layer]={\n",
    "        'corr_mat_whole':corr_mat_whole,\n",
    "        'all_embeddings':all_embeddings,\n",
    "        'all_labels_mat':all_labels_mat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 37/12543 [00:00<00:34, 365.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 429.82it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 311.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 503.80it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 426.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 526.32it/s]\n",
      "[computing labels]:   3%|▎         | 344/12543 [00:00<00:03, 3439.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5081.43it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6926.28it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7089.01it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 37/12543 [00:00<00:34, 362.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 432.33it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 333.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 510.14it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 421.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 526.66it/s]\n",
      "[computing labels]:   3%|▎         | 354/12543 [00:00<00:03, 3537.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5402.32it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6813.90it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7246.55it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:34, 357.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:28<00:00, 434.63it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 335.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 522.77it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 424.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 530.47it/s]\n",
      "[computing labels]:   2%|▏         | 310/12543 [00:00<00:03, 3099.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5158.91it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7021.92it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7351.60it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 36/12543 [00:00<00:35, 356.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:28<00:00, 434.09it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 34/2002 [00:00<00:05, 334.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 508.59it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 42/2077 [00:00<00:04, 418.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 524.96it/s]\n",
      "[computing labels]:   3%|▎         | 340/12543 [00:00<00:03, 3399.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5229.44it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7045.07it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7240.47it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 35/12543 [00:00<00:35, 348.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 428.32it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 31/2002 [00:00<00:06, 306.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 505.18it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 421.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 524.20it/s]\n",
      "[computing labels]:   3%|▎         | 356/12543 [00:00<00:03, 3557.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5051.43it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7079.49it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7027.40it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 34/12543 [00:00<00:37, 335.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:29<00:00, 426.92it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 32/2002 [00:00<00:06, 318.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 508.02it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 43/2077 [00:00<00:04, 416.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 524.52it/s]\n",
      "[computing labels]:   3%|▎         | 333/12543 [00:00<00:03, 3326.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5409.17it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 6844.23it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7305.30it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-train.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   0%|          | 37/12543 [00:00<00:34, 364.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 12543/12543 [00:28<00:00, 440.95it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-dev.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 31/2002 [00:00<00:06, 300.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2002/2002 [00:03<00:00, 522.32it/s]\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Pretrained Embeddings from ../example/data/en_ewt-ud/en_ewt-ud-test.bert-base-layers.hdf5; using layer 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]:   2%|▏         | 44/2077 [00:00<00:04, 438.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT-base-cased tokenizer to align embeddings with PTB tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[aligning embeddings]: 100%|██████████| 2077/2077 [00:03<00:00, 542.23it/s]\n",
      "[computing labels]:   3%|▎         | 365/12543 [00:00<00:03, 3648.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPTED 0\n",
      "Retaining 12543 training observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[computing labels]: 100%|██████████| 12543/12543 [00:02<00:00, 5497.33it/s]\n",
      "[computing labels]: 100%|██████████| 2002/2002 [00:00<00:00, 7247.71it/s]\n",
      "[computing labels]: 100%|██████████| 2077/2077 [00:00<00:00, 7300.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for layer in [2,4,5,7,8,10,11]:\n",
    "\n",
    "    corr_mat_whole,all_embeddings,all_labels_mat=get_corr_map(layer=layer)\n",
    "    corr_map_layer_dict[layer]={\n",
    "        'corr_mat_whole':corr_mat_whole,\n",
    "        'all_embeddings':all_embeddings,\n",
    "        'all_labels_mat':all_labels_mat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  1 0.9344386443610329 -0.44777127433163433 0.05445517548586958 0.045042605675376095\n",
      "layer:  3 0.5388708180055392 -0.3891944787099866 0.04346215384211578 0.03648069088313568\n",
      "layer:  6 0.40894972665598045 -0.46745444691679733 0.040155630284109615 0.0335834003641113\n",
      "layer:  9 0.29406288335072756 -0.5437658693230691 0.037730351200416595 0.03145815035875132\n",
      "layer:  12 0.8915102125630212 -0.9125223359281933 0.062358668342203924 0.050669063341169114\n"
     ]
    }
   ],
   "source": [
    "for layer in [1,3,6,9,12]:\n",
    "#     emb_mat = np.absolute(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    emb_mat = corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int)\n",
    "\n",
    "    print('layer: ',layer,np.max(emb_mat),np.min(emb_mat),np.mean(np.absolute(emb_mat)),np.median(np.absolute(emb_mat)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dim_from_file(file):\n",
    "    with open(file,'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "        line=lines[0].strip().split()\n",
    "        dims=[int(line[i]) for i in range(len(line))]\n",
    "    return dims\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer\t1\tmax-corr\t0.9344386443610329\tmean-corr\t0.05445517548586958 \n",
      "layer\t2\tmax-corr\t0.6790318898745872\tmean-corr\t0.045984754656778135 \n",
      "layer\t3\tmax-corr\t0.5388708180055392\tmean-corr\t0.04346215384211578 \n",
      "layer\t4\tmax-corr\t0.45349813123061394\tmean-corr\t0.04306083598380115 \n",
      "layer\t5\tmax-corr\t0.4081479121011531\tmean-corr\t0.0412659191877765 \n",
      "layer\t6\tmax-corr\t0.46745444691679733\tmean-corr\t0.040155630284109615 \n",
      "layer\t7\tmax-corr\t0.5212013234748754\tmean-corr\t0.03904785366661185 \n",
      "layer\t8\tmax-corr\t0.5658149898225272\tmean-corr\t0.03822041388538651 \n",
      "layer\t9\tmax-corr\t0.5437658693230691\tmean-corr\t0.037730351200416595 \n",
      "layer\t10\tmax-corr\t0.7716978112891458\tmean-corr\t0.03926388344173654 \n",
      "layer\t11\tmax-corr\t0.7371881250346611\tmean-corr\t0.041128799093268414 \n",
      "layer\t12\tmax-corr\t0.9125223359281933\tmean-corr\t0.062358668342203924 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for layer in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "    k=4\n",
    "#     emb_mat = np.absolute(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    emb_mat = np.array(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    dim_file = '../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer)\n",
    "    print('layer\\t{}\\tmax-corr\\t{}\\tmean-corr\\t{} '.format(layer,np.max(np.absolute(emb_mat)),np.mean(np.absolute(emb_mat)),))\n",
    "#     print('layer: ',layer,np.max(emb_mat),np.min(emb_mat),np.max(np.absolute(emb_mat)),np.mean(np.absolute(emb_mat)),np.median(np.absolute(emb_mat)))\n",
    "    dims=get_top_dim_from_file(file=dim_file)\n",
    "    top_emb_mat = emb_mat[dims[0:k]]\n",
    "    top_emb_mat = top_emb_mat[:,dims[0:k]]\n",
    "#     print('layer: ',layer,np.max(top_emb_mat),np.min(top_emb_mat),np.max(np.absolute(top_emb_mat)),np.mean(np.absolute(top_emb_mat)),np.median(np.absolute(top_emb_mat)))\n",
    "#     print(top_emb_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer\t1\tmax-corr\t0.9344386443610329\tmean-corr\t0.33377593890588725 \n",
      "layer\t2\tmax-corr\t0.6790318898745872\tmean-corr\t0.2774187195087825 \n",
      "layer\t3\tmax-corr\t0.5388708180055392\tmean-corr\t0.13675694176581563 \n",
      "layer\t4\tmax-corr\t0.45349813123061394\tmean-corr\t0.11756843120368758 \n",
      "layer\t5\tmax-corr\t0.3974267527739602\tmean-corr\t0.14666693370442338 \n",
      "layer\t6\tmax-corr\t0.40894972665598045\tmean-corr\t0.1272581291682241 \n",
      "layer\t7\tmax-corr\t0.3823351255817417\tmean-corr\t0.12069361046873045 \n",
      "layer\t8\tmax-corr\t0.31555230120604344\tmean-corr\t0.11244593569842101 \n",
      "layer\t9\tmax-corr\t0.2464554089840436\tmean-corr\t0.1098964963130136 \n",
      "layer\t10\tmax-corr\t0.2551217223482187\tmean-corr\t0.12510328069650808 \n",
      "layer\t11\tmax-corr\t0.21970662681363654\tmean-corr\t0.08209500617508024 \n",
      "layer\t12\tmax-corr\t0.18384732208722365\tmean-corr\t0.06815413873166584 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for layer in [1,2,3,4,5,6,7,8,9,10,11,12]:\n",
    "    k=5\n",
    "#     emb_mat = np.absolute(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    emb_mat = np.array(corr_map_layer_dict[layer]['corr_mat_whole'][50:,50:]-np.eye(768, dtype=int))\n",
    "    dim_file = '../SA-dim-files/average_corr_dim_layer_{}.tsv'.format(layer)\n",
    "#     print('layer: ',layer,np.max(emb_mat),np.min(emb_mat),np.max(np.absolute(emb_mat)),np.mean(np.absolute(emb_mat)),np.median(np.absolute(emb_mat)))\n",
    "    dims=get_top_dim_from_file(file=dim_file)\n",
    "    top_emb_mat = emb_mat[dims[0:k]]\n",
    "    top_emb_mat = top_emb_mat[:,dims[0:k]]\n",
    "    print('layer\\t{}\\tmax-corr\\t{}\\tmean-corr\\t{} '.format(layer,np.max(np.absolute(top_emb_mat)),np.mean(np.absolute(top_emb_mat)),))\n",
    "#     print('layer: ',layer,np.max(top_emb_mat),np.min(top_emb_mat),np.max(np.absolute(top_emb_mat)),np.mean(np.absolute(top_emb_mat)),np.median(np.absolute(top_emb_mat)))\n",
    "#     print(top_emb_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
